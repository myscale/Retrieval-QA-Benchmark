{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval QA Benchmark\n",
    "\n",
    "Retreival QA Benchmark (RQABench in short) is an open-sourced, end-to-end test workbench for Retrieval Augmented Generation (RAG) systems. We intend to build an open benchmark for all developers and researchers to reproduce and design new RAG systems. We also want to create a platform for everyone to share their lego blocks to help others to build up their own retrieval + LLM system.\n",
    "\n",
    "Here are some major feature of this benchmark:\n",
    "\n",
    "- **Flexibility**: We maximize the flexibility when design your retrieval system, as long as your transform accept `QARecord` as input and `QARecord` as output.\n",
    "- **Reproducibility**: We gather all settings in the evaluation process into a single YAML configuration. It helps you to track and reproduce experiements.\n",
    "- **Traceability**: We collect more than the accuracy and scores. We also focus on running times on any function you want to watch and the tokens used in the whole RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval_qa_benchmark.models import *\n",
    "from retrieval_qa_benchmark.datasets import *\n",
    "from retrieval_qa_benchmark.transforms import *\n",
    "from retrieval_qa_benchmark.evaluators import *\n",
    "from retrieval_qa_benchmark.utils.registry import REGISTRY\n",
    "from retrieval_qa_benchmark.utils.profiler import PROFILER\n",
    "\n",
    "print(str(REGISTRY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from retrieval_qa_benchmark.utils.factory import EvaluatorFactory\n",
    "from retrieval_qa_benchmark.utils.config import load\n",
    "\n",
    "config = load(open(\"../config/mmlu.yaml\"))\n",
    "evaluator = EvaluatorFactory.from_config(config).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocess.pool import ThreadPool\n",
    "from retrieval_qa_benchmark.utils.profiler import PROFILER\n",
    "\n",
    "PROFILER.clear()\n",
    "\n",
    "# shrink the size of the dataset\n",
    "evaluator.dataset.eval_set = evaluator.dataset.eval_set[:5]\n",
    "\n",
    "data = []\n",
    "for r in tqdm(\n",
    "    map(evaluator.transform, evaluator.dataset.iterator()), total=len(evaluator.dataset)\n",
    "):\n",
    "    data.append(r)\n",
    "with open(\"new_aligned.jsonl\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([json.dumps(d.model_dump()) for d in data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how it is formed as plain prompt\n",
    "print(evaluator.llm.convert_record(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print profile result\n",
    "print(str(PROFILER))\n",
    "PROFILER.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, result = evaluator()\n",
    "\n",
    "with open(\"results.with-retrieval.test.jsonl\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([r.model_dump_json() for r in result]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all mismatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval_qa_benchmark.schema import QAPrediction\n",
    "\n",
    "mismatched = [pred for pred in matched[1] if pred.matched]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.test.jsonl\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([r.model_dump_json() for r in mismatched]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dname, dset in REGISTRY.Datasets.items():\n",
    "    print(f\"Loading {dname}...\")\n",
    "    dset.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check intersection of two results (retreival system recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"old_aligned.jsonl\") as f1:\n",
    "    old_result = [json.loads(l) for l in f1.readlines()]\n",
    "\n",
    "with open(\"new.jsonl\") as f2:\n",
    "    new_result = [json.loads(l) for l in f2.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha256\n",
    "\n",
    "cnt = 0\n",
    "for r1, r2 in zip(old_result, new_result):\n",
    "    c1 = set(map(lambda x: sha256(x.encode(\"utf-8\")).hexdigest(), r1[\"context\"]))\n",
    "    c2 = set(map(lambda x: sha256(x.encode(\"utf-8\")).hexdigest(), r2[\"context\"]))\n",
    "    cnt += len(c2.intersection(c1))\n",
    "print(cnt / len(new_result) / 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
