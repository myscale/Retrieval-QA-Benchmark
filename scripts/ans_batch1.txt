thread: 4, context: 1
P4195 2023-09-06 11:43:21,208 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-1.jsonl
P4195 2023-09-06 11:53:45,364 INFO [bench_model.py:37] Throughput: 1.6 req/s, 438 prompt tokens/s (avg length 273), 160 completion tokens/s (avg_length 100)
thread: 4, context: 2
P4278 2023-09-06 11:53:46,005 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-2.jsonl
P4278 2023-09-06 12:05:00,257 INFO [bench_model.py:37] Throughput: 1.5 req/s, 628 prompt tokens/s (avg length 423), 148 completion tokens/s (avg_length 100)
thread: 4, context: 3
P4350 2023-09-06 12:05:00,908 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-3.jsonl
P4350 2023-09-06 12:17:06,472 INFO [bench_model.py:37] Throughput: 1.4 req/s, 794 prompt tokens/s (avg length 576), 137 completion tokens/s (avg_length 100)
thread: 4, context: 5
P4422 2023-09-06 12:17:07,106 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-5.jsonl
P4422 2023-09-06 12:31:00,389 INFO [bench_model.py:37] Throughput: 1.2 req/s, 1056 prompt tokens/s (avg length 880), 120 completion tokens/s (avg_length 100)
thread: 4, context: 10
P4494 2023-09-06 12:31:01,036 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-10.jsonl
P4494 2023-09-06 12:49:11,795 INFO [bench_model.py:37] Throughput: 0.9 req/s, 1484 prompt tokens/s (avg length 1619), 91 completion tokens/s (avg_length 100)
thread: 8, context: 1
P4566 2023-09-06 12:49:12,414 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-1.jsonl
P4566 2023-09-06 12:55:04,099 INFO [bench_model.py:37] Throughput: 2.8 req/s, 778 prompt tokens/s (avg length 273), 284 completion tokens/s (avg_length 100)
thread: 8, context: 2
P4642 2023-09-06 12:55:04,811 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-2.jsonl
P4642 2023-09-06 13:01:39,061 INFO [bench_model.py:37] Throughput: 2.5 req/s, 1074 prompt tokens/s (avg length 423), 253 completion tokens/s (avg_length 100)
thread: 8, context: 3
P4718 2023-09-06 13:01:39,708 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-3.jsonl
P4718 2023-09-06 13:08:59,051 INFO [bench_model.py:37] Throughput: 2.3 req/s, 1312 prompt tokens/s (avg length 576), 227 completion tokens/s (avg_length 100)
thread: 8, context: 5
P4794 2023-09-06 13:08:59,698 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-5.jsonl
P4794 2023-09-06 13:17:45,537 INFO [bench_model.py:37] Throughput: 1.9 req/s, 1674 prompt tokens/s (avg length 880), 190 completion tokens/s (avg_length 100)
thread: 8, context: 10
P4870 2023-09-06 13:17:46,216 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-10.jsonl
P4870 2023-09-06 13:30:34,871 INFO [bench_model.py:37] Throughput: 1.3 req/s, 2107 prompt tokens/s (avg length 1619), 130 completion tokens/s (avg_length 100)
thread: 16, context: 1
P4946 2023-09-06 13:30:35,536 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-1.jsonl
P4946 2023-09-06 13:34:16,534 INFO [bench_model.py:37] Throughput: 4.5 req/s, 1238 prompt tokens/s (avg length 273), 452 completion tokens/s (avg_length 100)
thread: 16, context: 2
P5030 2023-09-06 13:34:17,210 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-2.jsonl
P5030 2023-09-06 13:38:42,968 INFO [bench_model.py:37] Throughput: 3.8 req/s, 1594 prompt tokens/s (avg length 423), 376 completion tokens/s (avg_length 100)
thread: 16, context: 3
P5114 2023-09-06 13:38:43,626 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-3.jsonl
P5114 2023-09-06 13:43:59,756 INFO [bench_model.py:37] Throughput: 3.2 req/s, 1824 prompt tokens/s (avg length 576), 316 completion tokens/s (avg_length 100)
thread: 16, context: 5
P5198 2023-09-06 13:44:00,412 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-5.jsonl
P5198 2023-09-06 13:50:32,909 INFO [bench_model.py:37] Throughput: 2.5 req/s, 2243 prompt tokens/s (avg length 880), 254 completion tokens/s (avg_length 100)
thread: 16, context: 10
P5282 2023-09-06 13:50:33,614 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-10.jsonl
P5282 2023-09-06 14:01:17,836 INFO [bench_model.py:37] Throughput: 1.6 req/s, 2514 prompt tokens/s (avg length 1619), 155 completion tokens/s (avg_length 100)
thread: 32, context: 1
P5366 2023-09-06 14:01:18,506 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-1.jsonl
P5366 2023-09-06 14:03:56,794 INFO [bench_model.py:37] Throughput: 6.3 req/s, 1729 prompt tokens/s (avg length 273), 631 completion tokens/s (avg_length 100)
thread: 32, context: 2
P5466 2023-09-06 14:03:57,503 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-2.jsonl
P5466 2023-09-06 14:07:12,349 INFO [bench_model.py:37] Throughput: 5.1 req/s, 2174 prompt tokens/s (avg length 423), 513 completion tokens/s (avg_length 100)
thread: 32, context: 3
P5566 2023-09-06 14:07:13,011 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-3.jsonl
P5566 2023-09-06 14:11:05,862 INFO [bench_model.py:37] Throughput: 4.3 req/s, 2476 prompt tokens/s (avg length 576), 429 completion tokens/s (avg_length 100)
thread: 32, context: 5
P5666 2023-09-06 14:11:06,514 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-5.jsonl
P5666 2023-09-06 14:16:38,784 INFO [bench_model.py:37] Throughput: 3.0 req/s, 2650 prompt tokens/s (avg length 880), 301 completion tokens/s (avg_length 100)
thread: 32, context: 10
P5766 2023-09-06 14:16:39,423 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-10.jsonl
P5766 2023-09-06 14:27:18,750 INFO [bench_model.py:37] Throughput: 1.6 req/s, 2533 prompt tokens/s (avg length 1619), 156 completion tokens/s (avg_length 100)
thread: 64, context: 1
P6663 2023-09-06 14:27:19,414 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-1.jsonl
P6663 2023-09-06 14:29:20,763 INFO [bench_model.py:37] Throughput: 8.2 req/s, 2256 prompt tokens/s (avg length 273), 824 completion tokens/s (avg_length 100)
thread: 64, context: 2
P6795 2023-09-06 14:29:21,404 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-2.jsonl
P6795 2023-09-06 14:32:05,444 INFO [bench_model.py:37] Throughput: 6.1 req/s, 2583 prompt tokens/s (avg length 423), 609 completion tokens/s (avg_length 100)
thread: 64, context: 3
P6927 2023-09-06 14:32:06,111 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-3.jsonl
P6927 2023-09-06 14:35:44,685 INFO [bench_model.py:37] Throughput: 4.6 req/s, 2638 prompt tokens/s (avg length 576), 457 completion tokens/s (avg_length 100)
thread: 64, context: 5
P7059 2023-09-06 14:35:45,404 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-5.jsonl
P7059 2023-09-06 14:41:18,635 INFO [bench_model.py:37] Throughput: 3.0 req/s, 2642 prompt tokens/s (avg length 880), 300 completion tokens/s (avg_length 100)
thread: 64, context: 10
P7191 2023-09-06 14:41:19,311 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-10.jsonl
P7191 2023-09-06 14:52:02,437 INFO [bench_model.py:37] Throughput: 1.6 req/s, 2518 prompt tokens/s (avg length 1619), 155 completion tokens/s (avg_length 100)
thread: 128, context: 1
P7435 2023-09-06 14:52:03,115 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-1.jsonl
P7435 2023-09-06 14:53:57,075 INFO [bench_model.py:37] Throughput: 8.8 req/s, 2402 prompt tokens/s (avg length 273), 877 completion tokens/s (avg_length 100)
thread: 128, context: 2
P7631 2023-09-06 14:53:57,810 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-2.jsonl
P7631 2023-09-06 14:56:42,049 INFO [bench_model.py:37] Throughput: 6.1 req/s, 2580 prompt tokens/s (avg length 423), 609 completion tokens/s (avg_length 100)
thread: 128, context: 3
P7827 2023-09-06 14:56:42,716 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-3.jsonl
P7827 2023-09-06 15:00:22,296 INFO [bench_model.py:37] Throughput: 4.6 req/s, 2626 prompt tokens/s (avg length 576), 455 completion tokens/s (avg_length 100)
thread: 128, context: 5
P8023 2023-09-06 15:00:23,007 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-5.jsonl
P8023 2023-09-06 15:05:55,354 INFO [bench_model.py:37] Throughput: 3.0 req/s, 2649 prompt tokens/s (avg length 880), 300 completion tokens/s (avg_length 100)
thread: 128, context: 10
P8219 2023-09-06 15:05:56,006 INFO [bench_model.py:45] Run RAG performance benchmark with config_file=model.yaml, model=llama-2-13B-ensemble-v5-a100, jsonl_files=results-tgi/mmlu*-10.jsonl
> /data/workspace/RQA/retrieval_qa_benchmark/utils/profiler.py(27)wrapper()
-> self.accumulator[name] += (t1 - t0) * 1000
(Pdb) --KeyboardInterrupt--
(Pdb) --KeyboardInterrupt--
(Pdb) --KeyboardInterrupt--
(Pdb) --KeyboardInterrupt--
(Pdb)
root@96e2080aabd6:/data/workspace/RQA# Connection to 100.65.15.117 closed.
Connection to ssh.runpod.io closed.